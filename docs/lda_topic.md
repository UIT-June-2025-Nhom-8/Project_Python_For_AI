# ğŸ“Š LDA Topic Modeling Report (Amazon Reviews)

## 1. Giá»›i thiá»‡u

Latent Dirichlet Allocation (LDA) lÃ  mÃ´ hÃ¬nh chá»§ Ä‘á» xÃ¡c suáº¥t nháº±m rÃºt trÃ­ch cÃ¡c chá»§ Ä‘á» tiá»m áº©n tá»« táº­p vÄƒn báº£n. Má»¥c tiÃªu á»Ÿ Ä‘Ã¢y: huáº¥n luyá»‡n LDA trÃªn Amazon Reviews vÃ  Ä‘Ã¡nh giÃ¡ báº±ng cÃ¡c chá»‰ sá»‘ xÃ¡c suáº¥t (perplexity, log-likelihood, log-perplexity) cÃ¹ng **coherence (c_v)** Ä‘á»ƒ Ä‘o má»©c Ä‘á»™ â€œmáº¡ch láº¡câ€ cá»§a topic.

---

## 2. Dá»¯ liá»‡u & Tiá»n xá»­ lÃ½

- **Nguá»“n**: Amazon Reviews (train/test Ä‘Ã£ chuáº©n bá»‹ sáºµn).
- **Tiá»n xá»­ lÃ½**:
  - LÃ m sáº¡ch, chuáº©n hÃ³a chá»¯ thÆ°á»ng, loáº¡i kÃ½ tá»± nhiá»…u.
  - Tokenize, remove stopwords, stemming (SnowballStemmer).
  - `normalized_input`: danh sÃ¡ch token (list[str]).
  - ÄÆ°a vÃ o LDA báº±ng **Bag-of-Words (CountVectorizer)** (khÃ´ng dÃ¹ng TF-IDF cho LDA).

---

## 3. MÃ´ hÃ¬nh & CÃ i Ä‘áº·t

- **Triá»ƒn khai**: class `LDATopicModeler` (wrapper cho `sklearn.decomposition.LatentDirichletAllocation`), há»— trá»£:
  - `fit/transform/fit_transform` trÃªn chuá»—i token Ä‘Ã£ tiá»n xá»­ lÃ½.
  - `get_top_words_per_topic` Ä‘á»ƒ láº¥y top-words má»—i topic.
- **ÄÃ¡nh giÃ¡**:
  - **Perplexity** (sklearn): cÃ ng tháº¥p cÃ ng tá»‘t.
  - **Log-likelihood** (sklearn): cÃ ng cao cÃ ng tá»‘t.
  - **Log-perplexity per-word**: tÃ­nh giÃ¡n tiáº¿p tá»« `-log_likelihood / tá»•ng_sá»‘_tá»«`.
  - **Coherence (c_v)**: tÃ­nh báº±ng `gensim` vá»›i `CoherenceModel(topics=..., texts=..., dictionary=...)`.
- **Vectorizer (CountVectorizer)**: `max_features=20000`, `min_df=5`, `max_df=0.7`.
- **Tham sá»‘ LDA** (online VB): `max_iter=20`, `learning_method="online"`, `random_state=42`.

---

## 4. Thiáº¿t káº¿ Thá»±c nghiá»‡m

- So sÃ¡nh theo sá»‘ topic: `n_topics âˆˆ {10, 11, 12, â€¦, 20}`.
- TÃ­nh trÃªn **train** vÃ  **test**:
  - Perplexity, Log-likelihood, Log-perplexity (per-word).
  - Coherence c_v (gensim).
  - Thá»i gian huáº¥n luyá»‡n (giÃ¢y), kÃ­ch thÆ°á»›c vocab, #doc train.

---

## 5. Káº¿t quáº£

| n_topics | Test Perplexity â†“ | Train Perplexity | Test Log-Perplx | Train Log-Perplx | Test Log-Lik â†‘ | Train Log-Lik â†‘ | Coherence c_v â†‘ | Fit (s) | Vocab | #Train docs |
| -------: | ----------------: | ---------------: | --------------: | ---------------: | -------------: | --------------: | :-------------: | ------: | ----: | ----------: |
|       10 |           3191.39 |          2172.43 |          8.0682 |           7.6836 |    -3.1461e+06 |     -3.0038e+07 |   **0.5075**    |  570.78 | 20000 |      100000 |
|       11 |           3255.38 |          2216.27 |          8.0881 |           7.7036 |    -3.1538e+06 |     -3.0116e+07 |     0.5029      |  575.62 | 20000 |      100000 |
|   **12** |           3295.08 |          2210.72 |          8.1002 |           7.7011 |    -3.1586e+06 |     -3.0106e+07 |   **0.5513**    |  557.73 | 20000 |      100000 |
|       13 |           3397.89 |          2267.37 |          8.1309 |           7.7264 |    -3.1705e+06 |     -3.0205e+07 |     0.5327      |  583.37 | 20000 |      100000 |
|       14 |           3478.26 |          2297.54 |          8.1543 |           7.7396 |    -3.1797e+06 |     -3.0257e+07 |     0.5070      |  601.41 | 20000 |      100000 |
|       15 |           3506.42 |          2321.64 |          8.1624 |           7.7500 |    -3.1828e+06 |     -3.0297e+07 |     0.5452      |  585.10 | 20000 |      100000 |
|       16 |           3609.87 |          2374.62 |          8.1914 |           7.7726 |    -3.1941e+06 |     -3.0386e+07 |     0.4978      |  630.74 | 20000 |      100000 |
|       17 |           3643.50 |          2407.39 |          8.2007 |           7.7863 |    -3.1978e+06 |     -3.0439e+07 |     0.4934      |  599.72 | 20000 |      100000 |
|       18 |           3692.20 |          2424.39 |          8.2140 |           7.7933 |    -3.2029e+06 |     -3.0467e+07 |     0.4917      |  655.47 | 20000 |      100000 |
|       19 |           3725.68 |          2435.36 |          8.2230 |           7.7979 |    -3.2065e+06 |     -3.0484e+07 |     0.5015      |  664.37 | 20000 |      100000 |
|       20 |           3879.74 |          2518.36 |          8.2635 |           7.8314 |    -3.2223e+06 |     -3.0615e+07 |     0.5099      |  696.07 | 20000 |      100000 |

**Nháº­n xÃ©t nhanh:**

- **Perplexity (test)** tháº¥p nháº¥t á»Ÿ **n_topics=10** â†’ tá»‘t nháº¥t theo tiÃªu chÃ­ xÃ¡c suáº¥t tá»•ng quÃ¡t hÃ³a.
- **Coherence (c_v)** cao nháº¥t á»Ÿ **n_topics=12** (**0.5513**) â†’ chá»§ Ä‘á» dá»… diá»…n giáº£i nháº¥t theo c_v.
- Khi tÄƒng sá»‘ topic:
  - Perplexity (test) **tÄƒng dáº§n** â†’ rá»§i ro overfitting/khÃ³ tá»•ng quÃ¡t.
  - Log-perplexity per-word (test) **tÄƒng** (xáº¥u hÆ¡n).
  - Thá»i gian huáº¥n luyá»‡n tÄƒng (tuyáº¿n tÃ­nhâ€“siÃªu tuyáº¿n tÃ­nh theo sá»‘ topic).
- **Trade-off**: n=10 tá»‘i Æ°u perplexity; n=12 tá»‘i Æ°u coherence. Chá»n theo má»¥c tiÃªu:
  - Æ¯u tiÃªn **fit xÃ¡c suáº¥t / generalization** â†’ **n=10**.
  - Æ¯u tiÃªn **tÃ­nh máº¡ch láº¡c/chá»§ Ä‘á» dá»… hiá»ƒu** â†’ **n=12**.

---

## 6. Diá»…n giáº£i Chá»§ Ä‘á» (n_topics = 12)

> LÆ°u Ã½: Top-words Ä‘Ã£ Ä‘Æ°á»£c stem (vÃ­ dá»¥: _movi_ ~ _movie_, _stori_ ~ _story_), nháº±m gá»™p cÃ¡c biáº¿n thá»ƒ tá»« vá»±ng.

**Top-words theo tá»«ng chá»§ Ä‘á»:**

- **Topic 01** â€” book, read, life, one, peopl, work, histori, world, interest, us
- **Topic 02** â€” work, game, use, product, one, get, would, buy, time, great
- **Topic 03** â€” like, dont, get, love, old, buy, money, go, one, time
- **Topic 04** â€” use, one, product, great, good, work, would, like, look, get
- **Topic 05** â€” album, song, cd, music, like, one, listen, great, sound, good
- **Topic 06** â€” love, famili, stori, life, beauti, great, live, fun, enjoy, wonder
- **Topic 07** â€” book, read, charact, stori, one, like, end, novel, bore, good
- **Topic 08** â€” book, use, inform, good, help, learn, need, would, author, look
- **Topic 09** â€” movi, film, watch, one, like, good, time, see, great, bad
- **Topic 10** â€” great, movi, good, best, one, fan, like, seri, action, star
- **Topic 11** â€” book, read, great, one, love, stori, time, like, good, would
- **Topic 12** â€” dvd, version, movi, video, qualiti, edit, pictur, buy, great, watch

**Gá»£i Ã½ gÃ¡n nhÃ£n nhanh (tÃ¹y bá»‘i cáº£nh):**

- Books/Reading: 01, 07, 08, 11
- General Product/Usage: 02, 04
- Sentiment/Purchase (chung chung): 03
- Music/Album: 05
- Family/Life Stories: 06
- Movies/Video: 09, 10, 12

> LÆ°u Ã½ thÃªm: Má»™t sá»‘ topic cÃ³ nhiá»u tá»« phá»• dá»¥ng (_one, like, good_), lÃ m giáº£m Ä‘á»™ sáº¯c nÃ©t. CÃ³ thá»ƒ giáº£m `max_df`, tÄƒng `min_df` hoáº·c tÄƒng `max_features` trong CountVectorizer Ä‘á»ƒ giáº£m nhiá»…u vÃ  lÃ m rÃµ ranh giá»›i chá»§ Ä‘á».

Gá»£i Ã½ gÃ¡n nhÃ£n (tÃ¹y ngá»¯ cáº£nh): **Books/Reading**, **General Product/Usage**, **Sentiment/Purchase**, **Music/Album**, **Movies/Video**â€¦ Má»™t sá»‘ topic váº«n chá»©a tá»« chung chung (_one, like, good_), lÃ m giáº£m Ä‘á»™ sáº¯c nÃ©t.

## 7. Káº¿t luáº­n: Lá»±a chá»n sá»‘ lÆ°á»£ng chá»§ Ä‘á» tá»‘i Æ°u

Dá»±a trÃªn báº£ng so sÃ¡nh:

- **Perplexity**: tÄƒng dáº§n khi `n_topics` tÄƒng â‡’ mÃ´ hÃ¬nh cÃ ng phá»©c táº¡p, khÃ³ tá»•ng quÃ¡t hÃ³a.
- **Coherence (c_v)**: cao nháº¥t táº¡i **n_topics = 12** (â‰ˆ0.55), cho tháº¥y má»©c gáº¯n káº¿t chá»§ Ä‘á» tá»‘t nháº¥t trong cÃ¡c mÃ´ hÃ¬nh thá»­ nghiá»‡m.
- **Log-likelihood**: xu hÆ°á»›ng giáº£m nhÆ°ng á»•n Ä‘á»‹nh quanh n=12â€“15.

Vá»›i cáº£ ba chá»‰ sá»‘, **n_topics = 12** lÃ  lá»±a chá»n há»£p lÃ½ vÃ  cÃ¢n báº±ng nháº¥t.

---

## 8. Háº¡n cháº¿ & Tháº£o luáº­n

- Perplexity trÃªn test tÄƒng theo sá»‘ topic â†’ lá»±a chá»n `n_topics` lá»›n **khÃ´ng** cáº£i thiá»‡n tá»•ng quÃ¡t hÃ³a.
- Coherence cao nháº¥t á»Ÿ n=12 nhÆ°ng khÃ´ng chÃªnh lá»‡ch nhiá»u so vá»›i n=10/15 â†’ Ä‘á»™ máº¡ch láº¡c cáº£i thiá»‡n cÃ³ giá»›i háº¡n.
- Dá»¯ liá»‡u review ngáº¯n, ngÃ´n ngá»¯ Ä‘a dáº¡ng, nhiá»u tá»« phá»• dá»¥ng â†’ LDA (BoW) khÃ³ náº¯m báº¯t semantics sÃ¢u.
- Náº¿u má»¥c tiÃªu lÃ  **topic dá»… diá»…n giáº£i**: coherence lÃ  thÆ°á»›c Ä‘o chÃ­nh; náº¿u má»¥c tiÃªu lÃ  **kháº£ nÄƒng mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t**: Æ°u tiÃªn perplexity/log-perplexity.

---
