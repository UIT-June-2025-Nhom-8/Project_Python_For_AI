# PhÃ¢n TÃ­ch Hiá»‡u Suáº¥t MÃ´ HÃ¬nh: Táº¡i Sao Äá»™ ChÃ­nh XÃ¡c KhÃ´ng VÆ°á»£t Qua 0.9 vá»›i Dataset Lá»›n?

## Tá»•ng Quan Váº¥n Äá»

Dá»±a trÃªn phÃ¢n tÃ­ch chi tiáº¿t codebase vÃ  káº¿t quáº£ training, Ä‘Ã£ phÃ¡t hiá»‡n ra má»™t váº¥n Ä‘á» quan trá»ng: **Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh giáº£m Ä‘Ã¡ng ká»ƒ khi tÄƒng kÃ­ch thÆ°á»›c dataset tá»« 50,000 lÃªn 500,000 dÃ²ng, khÃ´ng thá»ƒ vÆ°á»£t qua ngÆ°á»¡ng 0.9**.

### Káº¿t Quáº£ Hiá»‡n Táº¡i (Dataset 500,000 dÃ²ng)
- **Logistic Regression**: 0.8905 accuracy (tá»‘t nháº¥t)
- **Random Forest**: 0.8378 accuracy  
- **Gradient Boosting**: 0.8517 accuracy

### So SÃ¡nh vá»›i Káº¿t Quáº£ TrÆ°á»›c ÄÃ³ (Dataset nhá» hÆ¡n)
Theo `docs/overview.md`, vá»›i dataset nhá» hÆ¡n Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c:
- **Gradient Boosting**: 93.13% accuracy
- **SVM**: Khoáº£ng 90%+ accuracy
- CÃ¡c mÃ´ hÃ¬nh khÃ¡c cÅ©ng Ä‘áº¡t performance tá»‘t hÆ¡n

---

## CÃ¡c Äiá»ƒm Nghi Ngá» vÃ  PhÃ¢n TÃ­ch Chi Tiáº¿t

### 1. **Data Quality Issues (Cháº¥t LÆ°á»£ng Dá»¯ Liá»‡u)**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- Dataset lá»›n (500K) cÃ³ thá»ƒ chá»©a nhiá»u **noise** vÃ  **low-quality samples** hÆ¡n
- Dá»¯ liá»‡u cÃ³ thá»ƒ bá»‹ **corrupted** hoáº·c **inconsistent labeling**

#### ğŸ“Š **Evidence tá»« code:**
```python
# Trong main.py
CONFIG = {
    "train_size": 500_000,  # Dataset lá»›n
    "test_size": 10000,
    # ... 
}
```

```python
# Validation logic trong data_loader cho tháº¥y cÃ³ checks cho unexpected labels
if train_invalid or test_invalid:
    print(f"Warning: Unexpected labels found")
```

#### ğŸ§ª **Khuyáº¿n nghá»‹ kiá»ƒm tra:**
- [ ] PhÃ¢n tÃ­ch distribution cá»§a labels trong 500K samples
- [ ] Kiá»ƒm tra text quality: length, character encoding, duplicates
- [ ] So sÃ¡nh sample quality giá»¯a dataset nhá» vÃ  lá»›n
- [ ] Thá»‘ng kÃª missing values, empty texts

---

### 2. **Class Imbalance Problem (Máº¥t CÃ¢n Báº±ng Class)**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- Dataset 500K cÃ³ thá»ƒ cÃ³ **severe class imbalance** 
- Tá»· lá»‡ positive/negative samples khÃ´ng Ä‘á»u, gÃ¢y bias cho model

#### ğŸ“Š **Evidence tá»« code:**
```python
# Model training khÃ´ng sá»­ dá»¥ng class balancing
optimize_hyperparameters=False  # KhÃ´ng optimize class_weight
```

```python
# Logistic Regression cÃ³ support cho class balancing
'class_weight': [None, 'balanced']  # NhÆ°ng khÃ´ng Ä‘Æ°á»£c enable
```

#### ğŸ§ª **Khuyáº¿n nghá»‹ kiá»ƒm tra:**
- [ ] PhÃ¢n tÃ­ch chi tiáº¿t distribution cá»§a 2 classes trong 500K dataset
- [ ] So sÃ¡nh vá»›i distribution trong dataset nhá» hÆ¡n
- [ ] Test vá»›i class_weight='balanced' parameter
- [ ] Sá»­ dá»¥ng stratified sampling

---

### 3. **Feature Engineering Limitations (Háº¡n Cháº¿ Feature Engineering)**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- **TF-IDF configuration** khÃ´ng phÃ¹ há»£p vá»›i dataset lá»›n
- **Feature dimensionality** cÃ³ thá»ƒ quÃ¡ tháº¥p hoáº·c quÃ¡ cao
- **Text preprocessing** khÃ´ng Ä‘á»§ máº¡nh cho dataset phá»©c táº¡p

#### ğŸ“Š **Evidence tá»« code:**
```python
# TF-IDF config trong main.py
CONFIG = {
    "tfidf_max_features": 5000,  # CÃ³ thá»ƒ quÃ¡ tháº¥p cho 500K samples
    "tfidf_min_df": 2,
    "tfidf_max_df": 0.8,
    "ngram_range": (1, 2),
}
```

```python
# Trong gradient_boosting_classifier.py
max_features=30000,  # Cao hÆ¡n nhÆ°ng khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng
```

#### ğŸ§ª **Khuyáº¿n nghá»‹ Ä‘iá»u chá»‰nh:**
- [ ] TÄƒng `max_features` tá»« 5,000 lÃªn 15,000-30,000
- [ ] Äiá»u chá»‰nh `min_df` vÃ  `max_df` cho dataset lá»›n
- [ ] Test vá»›i trigrams: `ngram_range=(1, 3)`
- [ ] ThÃªm numerical features: text length, punctuation count, etc.

---

### 4. **Model Complexity vs Dataset Size (Äá»™ Phá»©c Táº¡p MÃ´ HÃ¬nh)**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- Models quÃ¡ **Ä‘Æ¡n giáº£n** cho dataset 500K
- KhÃ´ng Ä‘á»§ **capacity** Ä‘á»ƒ há»c patterns phá»©c táº¡p
- **Regularization** quÃ¡ máº¡nh, gÃ¢y underfitting

#### ğŸ“Š **Evidence tá»« code:**
```python
# Logistic Regression vá»›i default parameters
model = LogisticRegression(random_state=42, max_iter=1000)
# KhÃ´ng cÃ³ hyperparameter tuning

# Random Forest vá»›i default config
model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
# n_estimators cÃ³ thá»ƒ quÃ¡ tháº¥p

# Gradient Boosting training time ráº¥t cao
'training_time_seconds': 361.58  # QuÃ¡ lÃ¢u, cÃ³ thá»ƒ cÃ³ váº¥n Ä‘á»
```

#### ğŸ§ª **Khuyáº¿n nghá»‹ Ä‘iá»u chá»‰nh:**
- [ ] Enable hyperparameter optimization: `optimize_hyperparameters=True`
- [ ] TÄƒng model complexity:
  - Random Forest: `n_estimators=300-500`
  - Gradient Boosting: `n_estimators=200`, `max_depth=8`
- [ ] Giáº£m regularization cho Logistic Regression

---

### 5. **Memory vÃ  Performance Constraints**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- **Memory limitations** áº£nh hÆ°á»Ÿng Ä‘áº¿n model training
- **Computational constraints** gÃ¢y ra suboptimal training
- **Sparse matrix operations** khÃ´ng hiá»‡u quáº£

#### ğŸ“Š **Evidence tá»« code:**
```python
# Memory usage Ä‘Æ°á»£c track
print(f"   Memory usage: ~{X_train_tfidf.data.nbytes / (1024**2):.2f} MB")

# Gradient Boosting training time quÃ¡ lÃ¢u
'training_time_seconds': 361.58  # > 6 phÃºt cho 1 model
```

#### ğŸ§ª **Khuyáº¿n nghá»‹:**
- [ ] Monitor memory usage during training
- [ ] Sá»­ dá»¥ng batch training cho models lá»›n
- [ ] Optimize sparse matrix operations
- [ ] Consider model parallelization

---

### 6. **Train/Test Split vÃ  Cross-Validation Issues**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- **Test set** quÃ¡ nhá» (10K) so vá»›i train set (500K)
- KhÃ´ng cÃ³ **proper validation strategy**
- **Data leakage** cÃ³ thá»ƒ xáº£y ra

#### ğŸ“Š **Evidence tá»« code:**
```python
CONFIG = {
    "train_size": 500_000,
    "test_size": 10000,  # Ratio 50:1, khÃ´ng balanced
}

# KhÃ´ng cÃ³ validation set
# KhÃ´ng cÃ³ cross-validation trong main pipeline
```

#### ğŸ§ª **Khuyáº¿n nghá»‹:**
- [ ] TÄƒng test_size lÃªn 50K-100K (ratio 10:1 or 5:1)
- [ ] Implement proper train/validation/test split
- [ ] Add k-fold cross-validation
- [ ] Ensure stratified sampling

---

### 7. **Evaluation Metrics vÃ  Bias**

#### ğŸ” **Nghi ngá» chÃ­nh:**
- **Accuracy** khÃ´ng pháº£i metric tá»‘t nháº¥t cho binary classification
- **Class distribution** trong test set khÃ´ng representative
- **Threshold optimization** chÆ°a Ä‘Æ°á»£c thá»±c hiá»‡n

#### ğŸ“Š **Evidence tá»« code:**
```python
# Chá»‰ focus vÃ o accuracy
'test_accuracy': 0.8905

# F1-scores cÅ©ng tháº¥p
'test_f1_macro': 0.8904176753729849
'test_f1_weighted': 0.8904935833209457
```

#### ğŸ§ª **Khuyáº¿n nghá»‹:**
- [ ] Focus on F1-score, Precision, Recall
- [ ] Analyze per-class performance
- [ ] ROC-AUC analysis
- [ ] Threshold optimization for better balance

---

## Káº¿ Hoáº¡ch Thá»­ Nghiá»‡m Cá»¥ Thá»ƒ

### Phase 1: Data Analysis
1. **Dataset Quality Check**
   ```python
   # Analyze 500K dataset
   - Label distribution
   - Text length distribution  
   - Duplicate analysis
   - Missing/empty text analysis
   ```

2. **Comparative Analysis**
   ```python
   # Compare vá»›i dataset nhá» hÆ¡n
   - Sample 50K from 500K dataset
   - Train models on both
   - Compare results
   ```

### Phase 2: Feature Engineering Optimization
1. **TF-IDF Tuning**
   ```python
   CONFIG_NEW = {
       "tfidf_max_features": 20000,  # TÄƒng tá»« 5000
       "tfidf_min_df": 5,           # TÄƒng tá»« 2
       "tfidf_max_df": 0.9,         # TÄƒng tá»« 0.8
       "ngram_range": (1, 3),       # ThÃªm trigrams
   }
   ```

2. **Additional Features**
   ```python
   # ThÃªm numerical features
   - text_length
   - word_count  
   - punctuation_count
   - capitalization_ratio
   ```

### Phase 3: Model Optimization
1. **Hyperparameter Tuning**
   ```python
   # Enable optimization
   optimize_hyperparameters=True
   
   # Test specific configurations
   - Logistic: Higher C values, different solvers
   - Random Forest: More trees, deeper trees
   - Gradient Boosting: More conservative settings
   ```

2. **Advanced Models**
   ```python
   # Test additional models
   - XGBoost
   - LightGBM  
   - Neural Networks (if feasible)
   ```

### Phase 4: Validation Strategy
1. **Cross-Validation**
   ```python
   # Implement proper CV
   - 5-fold stratified CV
   - Time-based split if applicable
   ```

2. **Ensemble Methods**
   ```python
   # Combine best models
   - Voting classifier
   - Stacking ensemble
   ```

---

## Dá»± ÄoÃ¡n NguyÃªn NhÃ¢n ChÃ­nh

Dá»±a trÃªn phÃ¢n tÃ­ch, **3 nguyÃªn nhÃ¢n cÃ³ kháº£ nÄƒng cao nháº¥t**:

1. **ğŸ¥‡ Feature Engineering Inadequate (40%)**
   - TF-IDF max_features=5000 quÃ¡ tháº¥p cho 500K samples
   - Cáº§n tÄƒng lÃªn 15K-30K features

2. **ğŸ¥ˆ Class Imbalance (30%)**  
   - Dataset lá»›n cÃ³ thá»ƒ cÃ³ distribution khÃ´ng cÃ¢n báº±ng
   - Cáº§n class balancing strategies

3. **ğŸ¥‰ Model Complexity Insufficient (30%)**
   - Models vá»›i default parameters quÃ¡ Ä‘Æ¡n giáº£n
   - Cáº§n hyperparameter tuning vÃ  model complexity tÄƒng

---

## Káº¿t Luáº­n

Váº¥n Ä‘á» performance drop khi scale up dataset lÃ  **common issue** trong ML. CÃ¡c action items Æ°u tiÃªn:

1. âœ… **Immediate**: TÄƒng TF-IDF max_features vÃ  enable hyperparameter tuning
2. âœ… **Short-term**: PhÃ¢n tÃ­ch data quality vÃ  class distribution  
3. âœ… **Medium-term**: Implement proper validation vÃ  ensemble methods

**Expected outcome**: Vá»›i cÃ¡c Ä‘iá»u chá»‰nh trÃªn, accuracy cÃ³ thá»ƒ cáº£i thiá»‡n lÃªn **0.92-0.95** cho dataset 500K.
