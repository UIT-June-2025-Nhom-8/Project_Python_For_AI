# -*- coding: utf-8 -*-
"""Python-Amazon-reviews

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6ngi-THHovngbr2evc5W83Du3YzbFGq

# Đồ Án Môn Lập Trình Python cho máy học

# Nhóm 8

*   25410056 - Lã Xuân Hồng - 25410056@ms.uit.edu.vn
*   25410034 - Lê Quang Hoài Đức - 25410034@ms.uit.edu.vn
*   25410150 - Nguyễn Minh Trọng - 25410150@ms.uit.edu.vn
*   25410088 - Trần Thanh Long - 25410088@ms.uit.edu.vn
*   25410104 - Nguyễn Minh Nhật - 25410104@ms.uit.edu.vn
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
kritanjalijain_amazon_reviews_path = kagglehub.dataset_download('kritanjalijain/amazon-reviews')

print('Data source import complete.')

"""# 1. Chuẩn bị data"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import tarfile # this is to extract the data from that .tgz file

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# get all of the data out of that .tgz
# amazon_reviews = tarfile.open('/kaggle/input/amazon-reviews/amazon_review_polarity_csv.tgz')
# amazon_reviews.extractall('data')
# amazon_reviews.close()

import os
for dirname, _, filenames in os.walk('./'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# 2. Khám phá và làm sạch tập data

*   Loại bỏ các giá trị rỗng (null) và không nhất quán
*   Kiểm tra các bản ghi trùng lặp trong dữ liệu
*   Tiền xử lý văn bản, loại bỏ stopwords, tách từ, và áp dụng kỹ thuật stemming (đưa từ về dạng gốc/dạng từ điển)




"""

import pandas as pd # Nhập thư viện pandas để xử lý và phân tích dữ liệu
import re # Nhập module re (biểu thức chính quy) để tìm kiếm và xử lý chuỗi
import string # Nhập module string để làm việc với các hằng số và hàm của chuỗi
import collections # Nhập module collections để sử dụng các kiểu dữ liệu container đặc biệt

import matplotlib.pyplot as plt # Nhập module pyplot từ matplotlib để vẽ đồ thị

import nltk # Nhập thư viện Natural Language Toolkit (NLTK)
from nltk.corpus import stopwords # Nhập corpus stopwords từ NLTK để truy cập danh sách các từ dừng
from nltk.tokenize import word_tokenize # Nhập hàm word_tokenize từ NLTK để tách văn bản thành từ
from nltk.stem import WordNetLemmatizer, SnowballStemmer # Nhập WordNetLemmatizer và SnowballStemmer để đưa từ về dạng gốc

nltk.download('punkt_tab')

train_df = pd.read_csv('./data/amazon_review_polarity_csv/train.csv')
test_df = pd.read_csv('./data/amazon_review_polarity_csv/test.csv')

train_df.columns = ['label', 'title', 'text']
train_df.head()

test_df.columns = ['label', 'title', 'text']
test_df.head()

print(train_df.head())

train_df.info()

train_df = train_df.head(100000)
test_df = test_df.head(10000)

"""**Loại bỏ các giá trị rỗng**"""

def clean_data(df):
    """
    Kiểm tra và xử lý giá trị null, đồng thời kiểm tra loại dữ liệu trong DataFrame.

    Args:
        df (pd.DataFrame): DataFrame cần làm sạch.

    Returns:
        pd.DataFrame: DataFrame đã được làm sạch.
    """
    print("Số lượng giá trị null trước khi xử lý:")
    print(df.isnull().sum())

    # Điền giá trị NaN trong cột 'text' bằng chuỗi rỗng
    if 'text' in df.columns:
        df['text'] = df['text'].fillna('')

    if 'title' in df.columns:
        df['title'] = df['title'].fillna('')

    print("\nSố lượng giá trị null sau khi xử lý:")
    print(df.isnull().sum())

    # Kiểm tra loại dữ liệu
    print("\nLoại dữ liệu của các cột:")
    print(df.dtypes)

    return df

# Áp dụng hàm làm sạch dữ liệu cho train_df và test_df
print("--- Xử lý train_df ---")
train_df = clean_data(train_df)

print("\n--- Xử lý test_df ---")
test_df = clean_data(test_df)

"""**Loại bỏ dữ liệu trùng lặp**"""

def remove_duplicates(df):
    """
    Kiểm tra và loại bỏ các bản ghi trùng lặp trong DataFrame.

    Args:
        df (pd.DataFrame): DataFrame cần xử lý.

    Returns:
        pd.DataFrame: DataFrame sau khi đã loại bỏ các bản ghi trùng lặp.
    """
    print(f"Số lượng bản ghi trước khi loại bỏ trùng lặp: {len(df)}")

    # Kiểm tra và loại bỏ các bản ghi trùng lặp
    df_cleaned = df.drop_duplicates()

    print(f"Số lượng bản ghi sau khi loại bỏ trùng lặp: {len(df_cleaned)}")

    return df_cleaned

# Áp dụng hàm remove_duplicates cho train_df và test_df
print("--- Xử lý train_df ---")
train_df = remove_duplicates(train_df)

print("\n--- Xử lý test_df ---")
test_df = remove_duplicates(test_df)

"""**Xóa ký tự không mong muốn**"""

def clean_text(text):
    """
    Làm sạch văn bản bằng cách:
    - Loại bỏ các ký tự đặc biệt và số.
    - Chuyển đổi sang chữ thường.
    - Loại bỏ dấu câu.
    - Loại bỏ khoảng trắng thừa.
    - Thay thế một hoặc nhiều khoảng trắng bằng một khoảng trắng duy nhất.
    - Loại bỏ khoảng trắng ở đầu/cuối.

    Args:
        text (str): Chuỗi văn bản cần làm sạch.

    Returns:
        str: Chuỗi văn bản đã được làm sạch.
    """
    # Loại bỏ các ký tự đặc biệt và số
    # Sử dụng regex để giữ lại chỉ các chữ cái (tiếng Anh và tiếng Việt có dấu) và dấu cách
    text = re.sub(r'[^A-Za-zÀ-ú ]+', '', text)
    # Chuyển đổi sang chữ thường
    text = text.lower()
    # loại bỏ dấu câu
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Loại bỏ khoảng trắng thừa
    # Thay thế một hoặc nhiều khoảng trắng bằng một khoảng trắng duy nhất và loại bỏ khoảng trắng ở đầu/cuối
    text = re.sub(r'\s+', ' ', text).strip()
    return text # Trả về chuỗi văn bản đã được làm sạch

# Áp dụng hàm clean_text cho train_df và test_df
print("--- Làm sạch train_df ---")
train_df['cleaned_title'] = train_df['title'].apply(clean_text)
train_df['cleaned_text'] = train_df['text'].apply(clean_text)
train_df.head()

print("\n--- Làm sạch test_df ---")
test_df['cleaned_title'] = test_df['title'].apply(clean_text)
test_df['cleaned_text'] = test_df['text'].apply(clean_text)
test_df.head()

"""**Tokenize text thành từng từ riêng lẻ**"""

def tokenize_text(text):
    """
    Tách văn bản thành các token (từ).

    Args:
        text (str): Chuỗi văn bản cần tách token.

    Returns:
        list: Danh sách các token.
    """
    # Sử dụng word_tokenize của NLTK để tách văn bản thành các token
    if isinstance(text, str):
      return word_tokenize(text)
    else:
      return text

print("--- Tokenize train_df ---")
train_df['tokenized_title'] = train_df['cleaned_title'].apply(tokenize_text)
train_df['tokenized_text'] = train_df['cleaned_text'].apply(tokenize_text)
train_df.head()

print("\n--- Làm sạch test_df ---")
test_df['tokenized_title'] = test_df['cleaned_title'].apply(tokenize_text)
test_df['tokenized_text'] = test_df['cleaned_text'].apply(tokenize_text)
test_df.head()

"""**Loại bỏ stopwords**"""

def remove_stopwords(tokens):
    """
    Loại bỏ các từ dừng (stopwords) tiếng Anh khỏi danh sách tokens.

    Args:
        filtered_tokens (list): Danh sách các token cần xử lý.

    Returns:
        list: Danh sách các token sau khi đã loại bỏ stopwords.
    """
    if not isinstance(tokens, list):
      return tokens
    else:
      # Tải danh sách các từ dừng tiếng Anh từ NLTK
      stop_words = set(stopwords.words('english'))
      # Lọc bỏ các từ dừng khỏi danh sách tokens
      filtered_tokens = [word for word in tokens if word not in stop_words]
      # Trả về danh sách các token đã lọc
      return filtered_tokens

# Áp dụng hàm remove_stopwords cho train_df và test_df
print("--- Xóa stopwords train_df ---")
train_df['no_stopwords_title'] = train_df['tokenized_title'].apply(remove_stopwords)
train_df['no_stopwords_text'] = train_df['tokenized_text'].apply(remove_stopwords)
train_df.head()

print("\n--- Xóa stopwords test_df ---")
test_df['no_stopwords_title'] = test_df['tokenized_title'].apply(remove_stopwords)
test_df['no_stopwords_text'] = test_df['tokenized_text'].apply(remove_stopwords)
test_df.head()

"""**Chuẩn hóa từ**"""

def normalize_token(tokens):
    """
    Chuẩn hóa danh sách tokens bằng cách áp dụng Snowball Stemmer tiếng Anh cho từng token.

    Args:
        tokens (list): Danh sách các token cần chuẩn hóa.

    Returns:
        list: Danh sách các token sau khi đã chuẩn hóa.
    """
    if not isinstance(tokens, list):
      return tokens
    else:
      # Khởi tạo Snowball Stemmer cho tiếng Anh
      stemmer = SnowballStemmer("english")
      # Áp dụng stemmer cho từng token trong danh sách và trả về danh sách mới
      normalized_tokens = [stemmer.stem(word) for word in tokens]
      return normalized_tokens

# Áp dụng hàm normalize_text cho train_df và test_df
print("--- Chuẩn hóa từ cho train_df ---")
train_df['normalized_title'] = train_df['no_stopwords_title'].apply(normalize_token)
train_df['normalized_text'] = train_df['no_stopwords_text'].apply(normalize_token)

print("\n--- Chuẩn hóa từ cho test_df ---")
test_df['normalized_title'] = test_df['no_stopwords_title'].apply(normalize_token)
test_df['normalized_text'] = test_df['no_stopwords_text'].apply(normalize_token)

train_df.head()

test_df.head()