{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08a5d1f7",
   "metadata": {},
   "source": [
    "# Amazon Reviews Sentiment Analysis - Pipeline\n",
    "\n",
    "This notebook contains a complete pipeline for Amazon reviews sentiment analysis including:\n",
    "- Data loading from Kaggle\n",
    "- Text preprocessing and cleaning\n",
    "- Text analysis and statistics\n",
    "- TF-IDF vectorization\n",
    "- Word cloud generation\n",
    "\n",
    "**Note for Google Colab users:**\n",
    "- Install required packages: `!pip install kagglehub wordcloud`\n",
    "- Upload your Kaggle API key if needed\n",
    "\n",
    "**Note for NotebookLLM users:**\n",
    "- Some interactive features may not work\n",
    "- Focus on the analysis results and code structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1efaf",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries (Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if using Google Colab\n",
    "# !pip install kagglehub wordcloud scikit-learn nltk\n",
    "# !pip install matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15ce43",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import kagglehub\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7337aa",
   "metadata": {},
   "source": [
    "## 3. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Configuration\n",
    "CONFIG = {\n",
    "    \"train_size\": 100000,\n",
    "    \"test_size\": 10000,\n",
    "    \"tfidf_max_features\": 5000,\n",
    "    \"tfidf_min_df\": 2,\n",
    "    \"tfidf_max_df\": 0.8,\n",
    "    \"ngram_range\": (1, 2),\n",
    "}\n",
    "\n",
    "print(\"=== AMAZON REVIEWS DATA PROCESSING PIPELINE ===\")\n",
    "print(f\"Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a18b0",
   "metadata": {},
   "source": [
    "## 4. KaggleDataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDataLoader:\n",
    "    \"\"\"\n",
    "    Class for loading and preparing Kaggle Amazon reviews dataset\n",
    "    \n",
    "    Features:\n",
    "    - Downloads dataset from Kaggle\n",
    "    - Loads CSV data with error handling\n",
    "    - Validates data structure and labels\n",
    "    - Applies size limits from configuration\n",
    "    - Combines title and text columns into unified input\n",
    "    - Delegates data quality processing to PreProcessor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.dataset_path = None\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        \"\"\"Download Amazon reviews dataset from Kaggle using kritanjalijain dataset\"\"\"\n",
    "        print(\"Downloading Kaggle Amazon reviews dataset...\")\n",
    "        \n",
    "        try:\n",
    "            if self.dataset_path is None:\n",
    "                self.dataset_path = kagglehub.dataset_download(\n",
    "                    \"kritanjalijain/amazon-reviews\"\n",
    "                )\n",
    "            print(f\"KaggleHub download path: {self.dataset_path}\")\n",
    "            return self.dataset_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading dataset: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def load_csv_data(self):\n",
    "        \"\"\"Load CSV data from the downloaded dataset\"\"\"\n",
    "        if self.dataset_path is None:\n",
    "            self.download_dataset()\n",
    "        \n",
    "        train_csv_path = os.path.join(self.dataset_path, \"train.csv\")\n",
    "        test_csv_path = os.path.join(self.dataset_path, \"test.csv\")\n",
    "        \n",
    "        print(\"\\n=== LOADING DATA ===\")\n",
    "        try:\n",
    "            self.train_df = pd.read_csv(train_csv_path)\n",
    "            self.test_df = pd.read_csv(test_csv_path)\n",
    "            print(f\"Successfully loaded data:\")\n",
    "            print(f\"   - Train: {self.train_df.shape}\")\n",
    "            print(f\"   - Test: {self.test_df.shape}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: Dataset files not found!\")\n",
    "            print(f\"   Expected paths: {train_csv_path}, {test_csv_path}\")\n",
    "            sys.exit(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        return self.train_df, self.test_df\n",
    "    \n",
    "    def prepare_dataframes(self):\n",
    "        \"\"\"Prepare and validate dataframes with streamlined processing\"\"\"\n",
    "        if self.train_df is None or self.test_df is None:\n",
    "            self.load_csv_data()\n",
    "        \n",
    "        self.train_df.columns = [\"label\", \"title\", \"text\"]\n",
    "        self.test_df.columns = [\"label\", \"title\", \"text\"]\n",
    "        \n",
    "        self.validate_data()\n",
    "        self.apply_size_limits()\n",
    "        self.clean_and_combine_data()\n",
    "        \n",
    "        return self.train_df, self.test_df\n",
    "    \n",
    "    def validate_data(self):\n",
    "        \"\"\"Validate loaded data and perform initial quality checks\"\"\"\n",
    "        print(\"\\n=== DATA VALIDATION ===\")\n",
    "        \n",
    "        print(f\"Initial Train data info:\")\n",
    "        print(f\"   - Shape: {self.train_df.shape}\")\n",
    "        print(f\"Initial Test data info:\")\n",
    "        print(f\"   - Shape: {self.test_df.shape}\")\n",
    "        \n",
    "        print(f\"\\nInitial label distribution:\")\n",
    "        print(f\"   Training: {self.train_df['label'].value_counts().to_dict()}\")\n",
    "        print(f\"   Test: {self.test_df['label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        train_labels = set(self.train_df[\"label\"].unique())\n",
    "        test_labels = set(self.test_df[\"label\"].unique())\n",
    "        expected_labels = {1, 2}  # Binary sentiment: 1=negative, 2=positive\n",
    "        \n",
    "        train_invalid = train_labels - expected_labels\n",
    "        test_invalid = test_labels - expected_labels\n",
    "        \n",
    "        if train_invalid or test_invalid:\n",
    "            print(f\"Warning: Unexpected labels found\")\n",
    "            if train_invalid:\n",
    "                print(f\"   Training unexpected labels: {train_invalid}\")\n",
    "            if test_invalid:\n",
    "                print(f\"   Test unexpected labels: {test_invalid}\")\n",
    "        else:\n",
    "            print(\"All labels are within expected range [1, 2]\")\n",
    "        \n",
    "        print(\"Initial data validation completed\")\n",
    "    \n",
    "    def clean_and_combine_data(self):\n",
    "        \"\"\"Combine title/text columns and perform basic data preparation\"\"\"\n",
    "        print(\"\\n=== DATA COMBINATION ===\")\n",
    "        \n",
    "        self.train_df[\"title\"] = self.train_df[\"title\"].fillna(\"\")\n",
    "        self.train_df[\"text\"] = self.train_df[\"text\"].fillna(\"\")\n",
    "        self.test_df[\"title\"] = self.test_df[\"title\"].fillna(\"\")\n",
    "        self.test_df[\"text\"] = self.test_df[\"text\"].fillna(\"\")\n",
    "        \n",
    "        def smart_combine(title, text):\n",
    "            title_clean = str(title).strip()\n",
    "            text_clean = str(text).strip()\n",
    "            \n",
    "            if title_clean and text_clean:\n",
    "                return f\"{title_clean} {text_clean}\"\n",
    "            elif title_clean:\n",
    "                return title_clean\n",
    "            elif text_clean:\n",
    "                return text_clean\n",
    "            else:\n",
    "                return \"\"\n",
    "        \n",
    "        print(\"Combining title and text columns...\")\n",
    "        self.train_df[\"input\"] = self.train_df.apply(\n",
    "            lambda row: smart_combine(row[\"title\"], row[\"text\"]), axis=1\n",
    "        )\n",
    "        self.test_df[\"input\"] = self.test_df.apply(\n",
    "            lambda row: smart_combine(row[\"title\"], row[\"text\"]), axis=1\n",
    "        )\n",
    "        \n",
    "        self.train_df = self.train_df.drop([\"title\", \"text\"], axis=1).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        self.test_df = self.test_df.drop([\"title\", \"text\"], axis=1).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Data combination completed:\")\n",
    "        print(f\"   Training: {self.train_df.shape}\")\n",
    "        print(f\"   Test: {self.test_df.shape}\")\n",
    "        print(\n",
    "            f\"   Average input length - Train: {self.train_df['input'].str.len().mean():.1f}, Test: {self.test_df['input'].str.len().mean():.1f}\"\n",
    "        )\n",
    "    \n",
    "    def apply_size_limits(self):\n",
    "        \"\"\"Apply size limits from configuration\"\"\"\n",
    "        print(f\"\\n=== APPLYING SIZE LIMITS ===\")\n",
    "        original_train_size = len(self.train_df)\n",
    "        original_test_size = len(self.test_df)\n",
    "        \n",
    "        self.train_df = self.train_df.iloc[: self.config[\"train_size\"]].copy()\n",
    "        self.test_df = self.test_df.iloc[: self.config[\"test_size\"]].copy()\n",
    "        \n",
    "        print(f\"Size limits applied:\")\n",
    "        print(f\"   Training: {original_train_size} -> {len(self.train_df)} samples\")\n",
    "        print(f\"   Test: {original_test_size} -> {len(self.test_df)} samples\")\n",
    "\n",
    "print(\"KaggleDataLoader class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc29b37",
   "metadata": {},
   "source": [
    "## 5. PreProcessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a34e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    # Advanced regex patterns for comprehensive text cleaning\n",
    "    CLEANING_PATTERNS = [\n",
    "        # Web content removal\n",
    "        (r\"http[s]?://\\S+|www\\.\\S+\", \"\"),  # URLs\n",
    "        (r\"\\S+@\\S+\\.\\S+\", \"\"),  # Email addresses\n",
    "        (r\"<[^>]+>\", \"\"),  # HTML tags\n",
    "        (r\"&[a-zA-Z0-9]+;\", \"\"),  # HTML entities\n",
    "        # Social media content\n",
    "        (r\"@\\w+|#\\w+\", \"\"),  # Mentions and hashtags\n",
    "        # Numbers and digits\n",
    "        (r\"\\d+\", \"\"),  # Remove all numbers\n",
    "        # Character filtering\n",
    "        (r\"[^a-zA-ZÀ-ÿĀ-žА-я\\u00C0-\\u017F\\u0100-\\u024F\\s]\", \"\"),  # Keep only letters\n",
    "        (r\"(.)\\1{2,}\", r\"\\1\\1\"),  # Repeated characters\n",
    "        (r\"\\s+\", \" \"),  # Normalize whitespace\n",
    "        (r\"\\b[b-hj-z]\\b\", \"\"),  # Single chars except a,i\n",
    "    ]\n",
    "\n",
    "    # Meaningful short words to preserve\n",
    "    MEANINGFUL_SHORT_WORDS = {\n",
    "        \"a\",\n",
    "        \"i\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"to\",\n",
    "        \"go\",\n",
    "        \"no\",\n",
    "        \"so\",\n",
    "        \"me\",\n",
    "        \"we\",\n",
    "        \"he\",\n",
    "        \"my\",\n",
    "        \"be\",\n",
    "        \"or\",\n",
    "        \"in\",\n",
    "        \"on\",\n",
    "        \"at\",\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Check and handle null values, and examine data types in DataFrame.\"\"\"\n",
    "        print(\"Number of null values before processing:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        # Fill NaN values in 'input' column with empty string\n",
    "        if \"input\" in df.columns:\n",
    "            df[\"input\"] = df[\"input\"].fillna(\"\")\n",
    "        elif \"text\" in df.columns:\n",
    "            df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "        elif \"title\" in df.columns:\n",
    "            df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "\n",
    "        print(\"\\nNumber of null values after processing:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        print(\"\\nData types of columns:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def remove_duplicates(self, df):\n",
    "        \"\"\"Check and remove duplicate records in DataFrame.\"\"\"\n",
    "        print(f\"Number of records before removing duplicates: {len(df)}\")\n",
    "\n",
    "        # Check and remove duplicate records\n",
    "        df_cleaned = df.drop_duplicates()\n",
    "\n",
    "        print(f\"Number of records after removing duplicates: {len(df_cleaned)}\")\n",
    "\n",
    "        return df_cleaned\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Comprehensive text cleaning with advanced preprocessing.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase first\n",
    "        text = text.lower()\n",
    "\n",
    "        # Apply all cleaning patterns in sequence\n",
    "        for pattern, replacement in self.CLEANING_PATTERNS:\n",
    "            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Strip whitespace after all pattern applications\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove very short words except meaningful ones\n",
    "        words = text.split()\n",
    "        words = [\n",
    "            word\n",
    "            for word in words\n",
    "            if len(word) >= 2 or word.lower() in self.MEANINGFUL_SHORT_WORDS\n",
    "        ]\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def preprocess_text_pipeline(self, text):\n",
    "        \"\"\"Complete text preprocessing pipeline that combines all steps efficiently.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "\n",
    "        # Step 1: Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "\n",
    "        # Step 2: Tokenize\n",
    "        tokens = self.tokenize_text(cleaned_text)\n",
    "\n",
    "        # Step 3: Remove stopwords\n",
    "        tokens_no_stopwords = self.remove_stopwords(tokens)\n",
    "\n",
    "        # Step 4: Normalize tokens\n",
    "        normalized_tokens = self.normalize_token(tokens_no_stopwords)\n",
    "\n",
    "        return normalized_tokens\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Split text into tokens (words).\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            return word_tokenize(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove English stopwords from the list of tokens.\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return tokens\n",
    "        else:\n",
    "            # Load English stopwords list from NLTK\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            # Filter out stopwords from the token list\n",
    "            filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "            # Return the filtered token list\n",
    "            return filtered_tokens\n",
    "\n",
    "    def normalize_token(self, tokens):\n",
    "        \"\"\"Normalize token list by applying English Snowball Stemmer to each token.\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return tokens\n",
    "        else:\n",
    "            # Initialize Snowball Stemmer for English\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            # Apply stemmer to each token in the list and return new list\n",
    "            normalized_tokens = [stemmer.stem(word) for word in tokens]\n",
    "            return normalized_tokens\n",
    "\n",
    "print(\"PreProcessor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c84bb",
   "metadata": {},
   "source": [
    "## 6. TextAnalyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    \"\"\"\n",
    "    Class for analyzing text data before TF-IDF vectorization\n",
    "\n",
    "    Features:\n",
    "    - Word frequency analysis\n",
    "    - Top word identification\n",
    "    - Word cloud generation\n",
    "    - Average word length calculation\n",
    "    - Text statistics and insights\n",
    "    - Dataset comparison\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_count = {}\n",
    "        self.total_words = 0\n",
    "        self.total_sentences = 0\n",
    "        self.analysis_results = {}\n",
    "\n",
    "    def analyze_word_count(self, sentence):\n",
    "        \"\"\"\n",
    "        Analyze word count for a single sentence\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence to analyze\n",
    "\n",
    "        Returns:\n",
    "            dict: Word count dictionary for the sentence\n",
    "        \"\"\"\n",
    "        word_count = {}\n",
    "        for word in sentence.split():\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        return word_count\n",
    "\n",
    "    def build_corpus_word_count(self, dataset, text_column=\"input\"):\n",
    "        \"\"\"\n",
    "        Build word count dictionary for entire corpus\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset containing text data\n",
    "            text_column (str): Column name containing text data\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete word count dictionary\n",
    "        \"\"\"\n",
    "        print(\"Building corpus word count...\")\n",
    "        self.word_count = {}\n",
    "\n",
    "        for sentence in dataset[text_column]:\n",
    "            if isinstance(sentence, str):\n",
    "                for word in sentence.split():\n",
    "                    if word in self.word_count:\n",
    "                        self.word_count[word] += 1\n",
    "                    else:\n",
    "                        self.word_count[word] = 1\n",
    "\n",
    "        self.total_words = sum(self.word_count.values())\n",
    "        self.total_sentences = len(dataset)\n",
    "\n",
    "        print(f\"   Total unique words: {len(self.word_count):,}\")\n",
    "        print(f\"   Total word occurrences: {self.total_words:,}\")\n",
    "        print(f\"   Total sentences: {self.total_sentences:,}\")\n",
    "\n",
    "        return self.word_count\n",
    "\n",
    "    def get_top_words(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Get top N most frequent words\n",
    "\n",
    "        Args:\n",
    "            top_n (int): Number of top words to return\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of top words with their counts\n",
    "        \"\"\"\n",
    "        if not self.word_count:\n",
    "            print(\n",
    "                \"Warning: Word count not built yet. Call build_corpus_word_count() first.\"\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "        top_words = dict(\n",
    "            sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTop {top_n} most frequent words:\")\n",
    "        for i, (word, count) in enumerate(top_words.items(), 1):\n",
    "            percentage = (count / self.total_words) * 100\n",
    "            print(f\"   {i:2d}. {word:15s} -> {count:6,} times ({percentage:.2f}%)\")\n",
    "\n",
    "        return top_words\n",
    "\n",
    "    def generate_wordcloud(\n",
    "        self,\n",
    "        dataset,\n",
    "        text_column=\"input\",\n",
    "        figsize=(12, 6),\n",
    "        remove_numbers=True,\n",
    "        save_path=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate and display word cloud from text data\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset containing text data\n",
    "            text_column (str): Column name containing text data\n",
    "            figsize (tuple): Figure size for the plot\n",
    "            remove_numbers (bool): Whether to remove numbers from text\n",
    "            save_path (str): Path to save the word cloud image\n",
    "        \"\"\"\n",
    "        print(\"Generating word cloud...\")\n",
    "\n",
    "        joined_sentences = \"\"\n",
    "        for sentence in dataset[text_column]:\n",
    "            if isinstance(sentence, str):\n",
    "                if remove_numbers:\n",
    "                    cleaned_sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "                else:\n",
    "                    cleaned_sentence = sentence\n",
    "                joined_sentences += \" \" + cleaned_sentence\n",
    "\n",
    "        if not joined_sentences.strip():\n",
    "            print(\"   Warning: No text data available for word cloud generation\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            wordcloud = WordCloud(\n",
    "                width=800,\n",
    "                height=400,\n",
    "                background_color=\"white\",\n",
    "                max_words=100,\n",
    "                colormap=\"viridis\",\n",
    "            ).generate(joined_sentences)\n",
    "\n",
    "            plt.figure(figsize=figsize)\n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\n",
    "                \"Word Cloud - Most Frequent Words\", fontsize=16, fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "            if save_path:\n",
    "                plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "                print(f\"   Word cloud saved to: {save_path}\")\n",
    "\n",
    "            plt.show()\n",
    "            print(\"   Word cloud generated successfully\")\n",
    "\n",
    "            return wordcloud\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Error generating word cloud: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_average_word_length(self, dataset, text_column=\"input\"):\n",
    "        \"\"\"\n",
    "        Calculate average word length across the dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset containing text data\n",
    "            text_column (str): Column name containing text data\n",
    "\n",
    "        Returns:\n",
    "            float: Average word length\n",
    "        \"\"\"\n",
    "        print(\"Calculating average word length...\")\n",
    "\n",
    "        total_length = 0\n",
    "        total_word_count = 0\n",
    "\n",
    "        for sentence in dataset[text_column]:\n",
    "            if isinstance(sentence, str):\n",
    "                words = sentence.split()\n",
    "                total_length += sum(len(word) for word in words)\n",
    "                total_word_count += len(words)\n",
    "\n",
    "        if total_word_count == 0:\n",
    "            print(\"   Warning: No words found in dataset\")\n",
    "            return 0.0\n",
    "\n",
    "        avg_word_length = round(total_length / total_word_count, 2)\n",
    "\n",
    "        print(f\"   Total characters: {total_length:,}\")\n",
    "        print(f\"   Total words: {total_word_count:,}\")\n",
    "        print(f\"   Average word length: {avg_word_length} characters\")\n",
    "\n",
    "        return avg_word_length\n",
    "\n",
    "    def analyze_text_statistics(self, dataset, text_column=\"input\"):\n",
    "        \"\"\"\n",
    "        Comprehensive text analysis including all statistics\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset containing text data\n",
    "            text_column (str): Column name containing text data\n",
    "\n",
    "        Returns:\n",
    "            dict: Complete analysis results\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== COMPREHENSIVE TEXT ANALYSIS ===\")\n",
    "        print(f\"Analyzing {len(dataset):,} text samples...\")\n",
    "\n",
    "        word_count = self.build_corpus_word_count(dataset, text_column)\n",
    "        top_10_words = self.get_top_words(10)\n",
    "        avg_word_length = self.calculate_average_word_length(dataset, text_column)\n",
    "\n",
    "        sentence_lengths = dataset[text_column].str.len()\n",
    "        word_counts_per_sentence = dataset[text_column].str.split().str.len()\n",
    "\n",
    "        self.analysis_results = {\n",
    "            \"corpus_statistics\": {\n",
    "                \"total_sentences\": len(dataset),\n",
    "                \"total_unique_words\": len(word_count),\n",
    "                \"total_word_occurrences\": self.total_words,\n",
    "                \"vocabulary_size\": len(word_count),\n",
    "            },\n",
    "            \"word_analysis\": {\n",
    "                \"top_10_words\": top_10_words,\n",
    "                \"average_word_length\": avg_word_length,\n",
    "                \"most_frequent_word\": (\n",
    "                    max(word_count.items(), key=lambda x: x[1])\n",
    "                    if word_count\n",
    "                    else (\"\", 0)\n",
    "                ),\n",
    "            },\n",
    "            \"sentence_statistics\": {\n",
    "                \"average_sentence_length\": round(sentence_lengths.mean(), 2),\n",
    "                \"median_sentence_length\": sentence_lengths.median(),\n",
    "                \"max_sentence_length\": sentence_lengths.max(),\n",
    "                \"min_sentence_length\": sentence_lengths.min(),\n",
    "                \"average_words_per_sentence\": round(word_counts_per_sentence.mean(), 2),\n",
    "            },\n",
    "            \"distribution_analysis\": {\n",
    "                \"words_appearing_once\": sum(\n",
    "                    1 for count in word_count.values() if count == 1\n",
    "                ),\n",
    "                \"words_appearing_more_than_10\": sum(\n",
    "                    1 for count in word_count.values() if count > 10\n",
    "                ),\n",
    "                \"words_appearing_more_than_100\": sum(\n",
    "                    1 for count in word_count.values() if count > 100\n",
    "                ),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        print(f\"\\n=== ANALYSIS SUMMARY ===\")\n",
    "        print(f\"Corpus Statistics:\")\n",
    "        for key, value in self.analysis_results[\"corpus_statistics\"].items():\n",
    "            print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "        print(f\"\\nWord Analysis:\")\n",
    "        print(f\"   Average word length: {avg_word_length} characters\")\n",
    "        print(\n",
    "            f\"   Most frequent word: '{self.analysis_results['word_analysis']['most_frequent_word'][0]}' ({self.analysis_results['word_analysis']['most_frequent_word'][1]:,} times)\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSentence Statistics:\")\n",
    "        for key, value in self.analysis_results[\"sentence_statistics\"].items():\n",
    "            print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "        print(f\"\\nDistribution Analysis:\")\n",
    "        for key, value in self.analysis_results[\"distribution_analysis\"].items():\n",
    "            print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "        return self.analysis_results\n",
    "\n",
    "    def get_word_frequency_report(self, min_frequency=1):\n",
    "        \"\"\"\n",
    "        Generate detailed word frequency report\n",
    "\n",
    "        Args:\n",
    "            min_frequency (int): Minimum frequency threshold for words to include\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Word frequency report as DataFrame\n",
    "        \"\"\"\n",
    "        if not self.word_count:\n",
    "            print(\n",
    "                \"Warning: Word count not built yet. Call build_corpus_word_count() first.\"\n",
    "            )\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        filtered_words = {\n",
    "            word: count\n",
    "            for word, count in self.word_count.items()\n",
    "            if count >= min_frequency\n",
    "        }\n",
    "\n",
    "        word_freq_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"word\": word,\n",
    "                    \"frequency\": count,\n",
    "                    \"percentage\": (count / self.total_words) * 100,\n",
    "                }\n",
    "                for word, count in sorted(\n",
    "                    filtered_words.items(), key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nWord Frequency Report (min frequency: {min_frequency}):\")\n",
    "        print(f\"   Words included: {len(word_freq_df):,}\")\n",
    "        if not word_freq_df.empty:\n",
    "            print(f\"   Coverage: {word_freq_df['percentage'].sum():.2f}% of total words\")\n",
    "\n",
    "        return word_freq_df\n",
    "\n",
    "    def compare_datasets(self, train_dataset, test_dataset, text_column=\"input\"):\n",
    "        \"\"\"\n",
    "        Compare text statistics between training and test datasets\n",
    "\n",
    "        Args:\n",
    "            train_dataset (pd.DataFrame): Training dataset\n",
    "            test_dataset (pd.DataFrame): Test dataset\n",
    "            text_column (str): Column name containing text data\n",
    "\n",
    "        Returns:\n",
    "            dict: Comparison results\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== DATASET COMPARISON ===\")\n",
    "\n",
    "        # Analyze training dataset (use current analyzer)\n",
    "        print(\"Analyzing training dataset...\")\n",
    "        train_results = self.analyze_text_statistics(train_dataset, text_column)\n",
    "\n",
    "        # Create new analyzer for test dataset to avoid conflicts\n",
    "        print(\"\\nAnalyzing test dataset...\")\n",
    "        test_analyzer = TextAnalyzer()\n",
    "        test_results = test_analyzer.analyze_text_statistics(test_dataset, text_column)\n",
    "\n",
    "        # Calculate comparison metrics\n",
    "        comparison = {\n",
    "            \"train_stats\": train_results,\n",
    "            \"test_stats\": test_results,\n",
    "            \"comparison\": {\n",
    "                \"vocabulary_size_ratio\": test_results[\"corpus_statistics\"][\n",
    "                    \"vocabulary_size\"\n",
    "                ]\n",
    "                / train_results[\"corpus_statistics\"][\"vocabulary_size\"],\n",
    "                \"avg_word_length_diff\": test_results[\"word_analysis\"][\n",
    "                    \"average_word_length\"\n",
    "                ]\n",
    "                - train_results[\"word_analysis\"][\"average_word_length\"],\n",
    "                \"avg_sentence_length_diff\": test_results[\"sentence_statistics\"][\n",
    "                    \"average_sentence_length\"\n",
    "                ]\n",
    "                - train_results[\"sentence_statistics\"][\"average_sentence_length\"],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Print comparison summary\n",
    "        print(f\"\\n=== DATASET COMPARISON SUMMARY ===\")\n",
    "        print(\n",
    "            f\"Vocabulary size ratio (test/train): {comparison['comparison']['vocabulary_size_ratio']:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Average word length difference: {comparison['comparison']['avg_word_length_diff']:.2f} characters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Average sentence length difference: {comparison['comparison']['avg_sentence_length_diff']:.2f} characters\"\n",
    "        )\n",
    "\n",
    "        return comparison\n",
    "\n",
    "print(\"TextAnalyzer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badaad6",
   "metadata": {},
   "source": [
    "## 7. TFIDFVectorizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad591b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFVectorizer:\n",
    "    def __init__(self, max_features=10000, min_df=2, max_df=0.8, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Initialize TF-IDF Vectorizer.\n",
    "\n",
    "        Args:\n",
    "            max_features (int): Maximum number of features\n",
    "            min_df (int): Minimum frequency of words in corpus\n",
    "            max_df (float): Maximum frequency of words in corpus (ratio)\n",
    "            ngram_range (tuple): N-gram range (1, 1) for unigram, (1, 2) for unigram + bigram\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words=\"english\",\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def preprocess_tokens_to_text(self, tokens):\n",
    "        \"\"\"Convert token list to text string.\"\"\"\n",
    "        if isinstance(tokens, list):\n",
    "            return \" \".join(tokens)\n",
    "        else:\n",
    "            return str(tokens)\n",
    "\n",
    "    def fit(self, text_data):\n",
    "        \"\"\"Train TF-IDF vectorizer on text data.\"\"\"\n",
    "        if isinstance(text_data, pd.Series):\n",
    "            processed_text = text_data.apply(self.preprocess_tokens_to_text)\n",
    "        else:\n",
    "            processed_text = [\n",
    "                self.preprocess_tokens_to_text(text) for text in text_data\n",
    "            ]\n",
    "\n",
    "        print(\"Training TF-IDF Vectorizer...\")\n",
    "        self.vectorizer.fit(processed_text)\n",
    "        self.is_fitted = True\n",
    "        print(\n",
    "            f\"Completed! Number of features: {len(self.vectorizer.get_feature_names_out())}\"\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, text_data):\n",
    "        \"\"\"Transform text data into TF-IDF matrix.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\n",
    "                \"Vectorizer has not been trained. Please call fit() method first.\"\n",
    "            )\n",
    "\n",
    "        if isinstance(text_data, pd.Series):\n",
    "            processed_text = text_data.apply(self.preprocess_tokens_to_text)\n",
    "        else:\n",
    "            processed_text = [\n",
    "                self.preprocess_tokens_to_text(text) for text in text_data\n",
    "            ]\n",
    "\n",
    "        print(\"Vectorizing data...\")\n",
    "        tfidf_matrix = self.vectorizer.transform(processed_text)\n",
    "        print(f\"Completed! Matrix shape: {tfidf_matrix.shape}\")\n",
    "        return tfidf_matrix\n",
    "\n",
    "    def fit_transform(self, text_data):\n",
    "        \"\"\"Train and transform data in one step.\"\"\"\n",
    "        return self.fit(text_data).transform(text_data)\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Get list of feature names.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\n",
    "                \"Vectorizer has not been trained. Please call fit() method first.\"\n",
    "            )\n",
    "\n",
    "        return self.vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "    def get_top_features(self, tfidf_matrix, top_n=20):\n",
    "        \"\"\"Get top N features with highest TF-IDF scores.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\n",
    "                \"Vectorizer has not been trained. Please call fit() method first.\"\n",
    "            )\n",
    "\n",
    "        mean_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        feature_names = self.get_feature_names()\n",
    "\n",
    "        feature_scores = list(zip(feature_names, mean_scores))\n",
    "        feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return feature_scores[:top_n]\n",
    "\n",
    "print(\"TFIDFVectorizer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250e508",
   "metadata": {},
   "source": [
    "## 8. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbae981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== INITIALIZING DATA LOADER ===\")\n",
    "data_loader = KaggleDataLoader(CONFIG)\n",
    "train_df, test_df = data_loader.prepare_dataframes()\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa32432",
   "metadata": {},
   "source": [
    "## 9. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"\\n=== TEXT PREPROCESSING ===\")\n",
    "print(\"Processing training data...\")\n",
    "train_df = preprocessor.clean_data(train_df.copy())\n",
    "train_df = preprocessor.remove_duplicates(train_df)\n",
    "# Use efficient pipeline method that combines cleaning, tokenization, stopword removal and normalization\n",
    "train_df = train_df.assign(\n",
    "    normalized_input=train_df[\"input\"].apply(preprocessor.preprocess_text_pipeline)\n",
    ")\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_df = preprocessor.clean_data(test_df.copy())\n",
    "test_df = preprocessor.remove_duplicates(test_df)\n",
    "# Use efficient pipeline method that combines cleaning, tokenization, stopword removal and normalization\n",
    "test_df = test_df.assign(\n",
    "    normalized_input=test_df[\"input\"].apply(preprocessor.preprocess_text_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ef2e5",
   "metadata": {},
   "source": [
    "## 10. Post-Preprocessing Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== POST-PREPROCESSING VALIDATION ===\")\n",
    "train_empty = (\n",
    "    train_df[\"normalized_input\"]\n",
    "    .apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    .eq(0)\n",
    "    .sum()\n",
    ")\n",
    "test_empty = (\n",
    "    test_df[\"normalized_input\"]\n",
    "    .apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    .eq(0)\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "print(f\"Training data quality:\")\n",
    "print(f\"   - Final shape: {train_df.shape}\")\n",
    "print(f\"   - Empty normalized_input: {train_empty}\")\n",
    "print(\n",
    "    f\"   - Average tokens per document: {train_df['normalized_input'].apply(len).mean():.2f}\"\n",
    ")\n",
    "\n",
    "print(f\"Test data quality:\")\n",
    "print(f\"   - Final shape: {test_df.shape}\")\n",
    "print(f\"   - Empty normalized_input: {test_empty}\")\n",
    "print(\n",
    "    f\"   - Average tokens per document: {test_df['normalized_input'].apply(len).mean():.2f}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSample processed data:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b660fc",
   "metadata": {},
   "source": [
    "## 11. Text Analysis Before TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ba64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TEXT ANALYSIS BEFORE TF-IDF VECTORIZATION ===\")\n",
    "text_analyzer = TextAnalyzer()\n",
    "\n",
    "print(\"\\n1. TRAINING DATA ANALYSIS\")\n",
    "train_analysis = text_analyzer.analyze_text_statistics(train_df, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219bad2",
   "metadata": {},
   "source": [
    "## 12. Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60355e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. WORD CLOUD GENERATION\")\n",
    "try:\n",
    "    text_analyzer.generate_wordcloud(\n",
    "        train_df, \"input\", figsize=(12, 6), save_path=\"wordcloud_train.png\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"   Could not generate word cloud: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44662714",
   "metadata": {},
   "source": [
    "## 13. Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3. DATASET COMPARISON\")\n",
    "comparison_results = text_analyzer.compare_datasets(train_df, test_df, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b86130",
   "metadata": {},
   "source": [
    "## 14. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. WORD FREQUENCY ANALYSIS\")\n",
    "word_freq_report = text_analyzer.get_word_frequency_report(min_frequency=5)\n",
    "if not word_freq_report.empty:\n",
    "    print(\"\\nTop 15 words with frequency >= 5:\")\n",
    "    print(word_freq_report.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986f7aa",
   "metadata": {},
   "source": [
    "## 15. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== TF-IDF VECTORIZATION ===\")\n",
    "tfidf_vectorizer = TFIDFVectorizer(\n",
    "    max_features=CONFIG[\"tfidf_max_features\"],\n",
    "    min_df=CONFIG[\"tfidf_min_df\"],\n",
    "    max_df=CONFIG[\"tfidf_max_df\"],\n",
    "    ngram_range=CONFIG[\"ngram_range\"],\n",
    ")\n",
    "print(f\"TF-IDF Configuration: {CONFIG}\")\n",
    "\n",
    "print(\"\\nTraining TF-IDF Vectorizer...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"normalized_input\"])\n",
    "\n",
    "print(\"Transforming test data...\")\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df[\"normalized_input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f05d3",
   "metadata": {},
   "source": [
    "## 16. TF-IDF Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a240a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== TF-IDF MATRIX ANALYSIS ===\")\n",
    "print(f\"Matrix Information:\")\n",
    "print(f\"   Train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"   Test shape: {X_test_tfidf.shape}\")\n",
    "print(\n",
    "    f\"   Sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])):.4f}\"\n",
    ")\n",
    "print(f\"   Memory usage: ~{X_train_tfidf.data.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\nTop 10 Most Important TF-IDF Features:\")\n",
    "try:\n",
    "    top_features = tfidf_vectorizer.get_top_features(X_train_tfidf, top_n=10)\n",
    "    for i, (feature, score) in enumerate(top_features, 1):\n",
    "        print(f\"   {i:2d}. {feature:20s} -> {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Could not extract top features: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2149ce5",
   "metadata": {},
   "source": [
    "## 17. Pipeline Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"PIPELINE COMPLETION SUMMARY\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Dataset Information:\")\n",
    "print(f\"   - Train samples: {len(train_df):,}\")\n",
    "print(f\"   - Test samples: {len(test_df):,}\")\n",
    "print(f\"   - Features: {X_train_tfidf.shape[1]:,}\")\n",
    "\n",
    "print(f\"\\nText Analysis Summary:\")\n",
    "if text_analyzer.analysis_results:\n",
    "    corpus_stats = text_analyzer.analysis_results[\"corpus_statistics\"]\n",
    "    word_stats = text_analyzer.analysis_results[\"word_analysis\"]\n",
    "    print(f\"   - Vocabulary size: {corpus_stats['vocabulary_size']:,}\")\n",
    "    print(f\"   - Total words: {corpus_stats['total_word_occurrences']:,}\")\n",
    "    print(f\"   - Average word length: {word_stats['average_word_length']} characters\")\n",
    "    print(\n",
    "        f\"   - Most frequent word: '{word_stats['most_frequent_word'][0]}' ({word_stats['most_frequent_word'][1]:,} times)\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "train_labels = train_df[\"label\"].value_counts()\n",
    "test_labels = test_df[\"label\"].value_counts()\n",
    "print(f\"   Train: {dict(train_labels)}\")\n",
    "print(f\"   Test:  {dict(test_labels)}\")\n",
    "\n",
    "print(f\"\\nData Ready for machine learning models:\")\n",
    "print(f\"   - X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"   - X_test_tfidf: {X_test_tfidf.shape}\")\n",
    "print(f\"   - y_train: {train_df['label'].shape}\")\n",
    "print(f\"   - y_test: {test_df['label'].shape}\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Prepare final variables for model training\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(\"\\nAll variables are ready for machine learning model training!\")\n",
    "print(\"Variables available:\")\n",
    "print(\"   - X_train_tfidf: Training features (TF-IDF matrix)\")\n",
    "print(\"   - X_test_tfidf: Test features (TF-IDF matrix)\")\n",
    "print(\"   - y_train: Training labels\")\n",
    "print(\"   - y_test: Test labels\")\n",
    "print(\"   - train_df: Original training dataframe\")\n",
    "print(\"   - test_df: Original test dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d867ab9",
   "metadata": {},
   "source": [
    "## 18. Next Steps for Model Training\n",
    "\n",
    "Now that the data is preprocessed and vectorized, you can proceed with machine learning model training. Here are some common approaches:\n",
    "\n",
    "### Option 1: Logistic Regression\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Option 2: Random Forest\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(f\"RF Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n",
    "```\n",
    "\n",
    "### Option 3: SVM\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm)}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
